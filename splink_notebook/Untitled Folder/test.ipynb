{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee8bef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Application Started\n",
      "dedupe\n"
     ]
    }
   ],
   "source": [
    "from future.builtins import next\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import logging\n",
    "import optparse\n",
    "from numpy import nan\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "import json\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType,FloatType\n",
    "\n",
    "# ## Logging\n",
    "\n",
    "# Dedupe uses Python logging to show or suppress verbose output. Added for convenience.\n",
    "# To enable verbose logging, run `python examples/csv_example.py -v`\n",
    "\n",
    "\n",
    "print(\"PySpark Application Started\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dedupe\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56737eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Application Started\n",
      "dedupe\n"
     ]
    }
   ],
   "source": [
    "print(\"PySpark Application Started\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Splink\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d999280",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/home/abhishekks446/data/output/10k_data.csv'\n",
    "training_file = '/home/abhishekks446/pyspark/my_project_dir/Untitled Folder/settings/csv_example_training.json'\n",
    "output_file = '/home/abhishekks446/pyspark/my_project_dir/Untitled Folder/settings/csv_example_output.csv'\n",
    "settings_file = '/home/abhishekks446/pyspark/my_project_dir/Untitled Folder/settings/csv_example_learned_settings'\n",
    "\n",
    "\n",
    "primart_key=\"unique_id\"\n",
    "\n",
    "\n",
    "df= spark.read.option(\"header\",\"true\").csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd73af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "    import unidecode\n",
    "    # column = column.decode(\"utf8\")\n",
    "    column = unidecode.unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column :\n",
    "        column = None\n",
    "    return column\n",
    "def readData():\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "    \n",
    "    data_d = {}\n",
    "    results = df.toJSON().map(lambda j: json.loads(j)).collect()\n",
    "    for row in results:\n",
    "        clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "        row_id = row[primart_key]\n",
    "        data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4220aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('importing data ...')\n",
    "data_d = readData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1207c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "{'field' : 'store_name', 'type': 'String'},\n",
    "{'field' : 'zip_code', 'type': 'String','has missing' : True},\n",
    "{'field' : 'address', 'type': 'Exact', 'has missing' : True}\n",
    "]\n",
    "deduper = dedupe.Dedupe(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098ee58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "from boto3 import client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a269bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUCKET = 'steerwise-bucket'\n",
    "READ_LABEL = 'dedupe/input_data/labelled.json'\n",
    "client = client('s3',\n",
    "                 aws_access_key_id='AKIA5FYUQOE6KKEFWNYH',\n",
    "                 aws_secret_access_key='8E2XYN1srVWQjo3lN+JNSbANFxEc9vDjBWwZxN3l'\n",
    "                )\n",
    "label_result = client.get_object(Bucket=BUCKET, Key=READ_LABEL) \n",
    "labeled_examples = label_result[\"Body\"]\n",
    "\n",
    "# data = json.loads(json_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04f16bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'field': 'store_name', 'type': 'String'}, {'field': 'zip_code', 'type': 'String', 'has missing': True}, {'field': 'address', 'type': 'Exact', 'has missing': True}]\n"
     ]
    }
   ],
   "source": [
    "argument_file = 'dedupe/input_data/app.json'\n",
    "result = client.get_object(Bucket=BUCKET, Key=argument_file) \n",
    "json_content = result[\"Body\"].read().decode()\n",
    "data = json.loads(json_content)\n",
    "input = data[\"input\"]\n",
    "output = data[\"output\"]\n",
    "l_fields = data[\"Fields\"]\n",
    "primary_key=data[\"primary_key\"]\n",
    "print(l_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "734e654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.prepare_training(data_d, json_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47af3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82d56a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_dupes = deduper.partition(data_d, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "058dd565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3407e508",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_membership = {}\n",
    "cluster_id = 0\n",
    "for (cluster_id, cluster) in enumerate(clustered_dupes):\n",
    "    id_set, scores = cluster\n",
    "    cluster_d = [data_d[c] for c in id_set]\n",
    "    canonical_rep = dedupe.canonicalize(cluster_d)\n",
    "    for record_id, score in zip(id_set, scores) :\n",
    "        cluster_membership[record_id] = {\n",
    "            \"cluster id\" : cluster_id,\n",
    "            \"canonical representation\" : canonical_rep,\n",
    "            \"confidence\": score\n",
    "        }\n",
    "\n",
    "singleton_id = cluster_id + 1\n",
    "\n",
    "reader= df\n",
    "ls=[]\n",
    "for row in reader.collect():\n",
    "    \n",
    "    row_id = (row[primart_key])\n",
    "    if row_id in cluster_membership :\n",
    "        cluster_id = cluster_membership[row_id][\"cluster id\"]\n",
    "        confidence_s=cluster_membership[row_id]['confidence']\n",
    "        g = str(float(\"{0:.2f}\".format(confidence_s)))\n",
    "        d_list=[row_id,cluster_id,g]\n",
    "\n",
    "        ls.append(d_list)\n",
    "\n",
    "# print(ls)\n",
    "columns = [\"row_id\", \"cluster_id\",\"confidence_score\"]    \n",
    "dataframe = spark.createDataFrame(ls, columns,FloatType())\n",
    "new_df=reader.join(dataframe ,reader.unique_id ==  dataframe.row_id,\"inner\")\n",
    "# new_df.show(40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86522a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186e0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
