{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72572970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Application Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/28 18:07:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedupe\n"
     ]
    }
   ],
   "source": [
    "from future.builtins import next\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import logging\n",
    "import optparse\n",
    "from numpy import nan\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "import json\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType,FloatType\n",
    "\n",
    "# ## Logging\n",
    "\n",
    "# Dedupe uses Python logging to show or suppress verbose output. Added for convenience.\n",
    "# To enable verbose logging, run `python examples/csv_example.py -v`\n",
    "\n",
    "\n",
    "print(\"PySpark Application Started\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dedupe\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce6e25eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/home/abhishekks446/data/BigBasket_Products.csv'\n",
    "training_file = '/home/abhishekks446/pyspark/my_project_dir/Untitled Folder/settings/csv_example_training.json'\n",
    "primart_key=\"unique_id\"\n",
    "\n",
    "df= spark.read.option(\"header\",\"true\").csv(input_file).withColumnRenamed(\"index\",\"unique_id\").limit(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf40c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unique_id: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- sale_price: string (nullable = true)\n",
      " |-- market_price: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f7a954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(\"/home/abhishekks446/data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8cbc358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "    import unidecode\n",
    "    # column = column.decode(\"utf8\")\n",
    "    column = unidecode.unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column :\n",
    "        column = None\n",
    "    return column\n",
    "def readData():\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "    \n",
    "    data_d = {}\n",
    "    results = df.toJSON().map(lambda j: json.loads(j)).collect()\n",
    "    for row in results:\n",
    "        clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "        row_id = row[primart_key]\n",
    "        data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef61781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
