{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72bfbddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Application Started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/08/17 11:28:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dedupe\n"
     ]
    }
   ],
   "source": [
    "from future.builtins import next\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import collections\n",
    "import logging\n",
    "import optparse\n",
    "from numpy import nan\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "import json\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType,FloatType\n",
    "\n",
    "# ## Logging\n",
    "\n",
    "# Dedupe uses Python logging to show or suppress verbose output. Added for convenience.\n",
    "# To enable verbose logging, run `python examples/csv_example.py -v`\n",
    "\n",
    "\n",
    "print(\"PySpark Application Started\")\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dedupe\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.appName)\n",
    "# ## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6997a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/home/abhishekks446/data/output/10k_data.csv'\n",
    "training_file = '/home/abhishekks446/pyspark/my_project_dir/Untitled Folder/settings/csv_example_training.json'\n",
    "output_file = '/home/abhishekks446/pyspark/my_project_dir/Untitled Folder/settings/csv_example_output.csv'\n",
    "settings_file = '/home/abhishekks446/pyspark/my_project_dir/Untitled Folder/settings/csv_example_learned_settings'\n",
    "\n",
    "\n",
    "primart_key=\"unique_id\"\n",
    "\n",
    "\n",
    "df= spark.read.option(\"header\",\"true\").csv(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc0023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "    import unidecode\n",
    "    # column = column.decode(\"utf8\")\n",
    "    column = unidecode.unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column :\n",
    "        column = None\n",
    "    return column\n",
    "def readData():\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "    \n",
    "    data_d = {}\n",
    "    dfn=df.na.fill(\"\")\n",
    "    results = df.toJSON().map(lambda j: json.loads(j)).collect()\n",
    "    for row in results:\n",
    "        clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "        row_id = row[primart_key]\n",
    "        data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12b78c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print('importing data ...')\n",
    "data_d = readData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a79ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f30d21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dedupe import labeler\n",
    "fields = [\n",
    "{'field' : 'store_name', 'type': 'String'},\n",
    "{'field' : 'zip_code', 'type': 'String','has missing' : True},\n",
    "{'field' : 'address', 'type': 'Exact', 'has missing' : True}\n",
    "]\n",
    "deduper = dedupe.Dedupe(fields)\n",
    "# labeler.sample(data_d,15000) \n",
    "\n",
    "# deduper.prepare_training(data_d)\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "825aad38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "from boto3 import client\n",
    "\n",
    "\n",
    "BUCKET = 'steerwise-bucket'\n",
    "FILE_TO_READ = 'dedupe/input_data/labelled.json'\n",
    "client = client('s3',\n",
    "                 aws_access_key_id='AKIA5FYUQOE6KKEFWNYH',\n",
    "                 aws_secret_access_key='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'\n",
    "                )\n",
    "result = client.get_object(Bucket=BUCKET, Key=FILE_TO_READ) \n",
    "json_content = result[\"Body\"]\n",
    "\n",
    "# data = json.loads(json_content)\n",
    "deduper.prepare_training(data_d, json_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2d1b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduper.mark_pairs(labeled_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea604fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedupe.console_label(deduper)\n",
    "# pair = deduper.uncertain_pairs()\n",
    "# print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ba50183",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "031d3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(training_file, 'w') as tf:\n",
    "#     deduper.write_training(tf)\n",
    "# with open(settings_file, 'wb') as sf:\n",
    "#     deduper.write_settings(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6307908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering...\n",
      "# duplicate sets 1581\n"
     ]
    }
   ],
   "source": [
    "print('clustering...')\n",
    "clustered_dupes = deduper.partition(data_d, .5)\n",
    "\n",
    "print('# duplicate sets', len(clustered_dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d0d5114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_membership = {}\n",
    "# for cluster_id, (records, scores) in enumerate(clustered_dupes):\n",
    "#     for record_id, score in zip(records, scores):\n",
    "#         cluster_membership[record_id] = {\n",
    "#             \"Cluster ID\": cluster_id,\n",
    "#             \"confidence_score\": score\n",
    "#         }\n",
    "\n",
    "# with open(output_file, 'w') as f_output, open(input_file) as f_input:\n",
    "\n",
    "#     reader = csv.DictReader(f_input)\n",
    "#     fieldnames = ['Cluster ID', 'confidence_score'] + reader.fieldnames\n",
    "\n",
    "#     writer = csv.DictWriter(f_output, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "\n",
    "#     for row in reader:\n",
    "#         row_id = row[primart_key]\n",
    "#         row.update(cluster_membership[row_id])\n",
    "#         writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e40dfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------outout\n"
     ]
    }
   ],
   "source": [
    "cluster_membership = {}\n",
    "cluster_id = 0\n",
    "for (cluster_id, cluster) in enumerate(clustered_dupes):\n",
    "    id_set, scores = cluster\n",
    "    cluster_d = [data_d[c] for c in id_set]\n",
    "    canonical_rep = dedupe.canonicalize(cluster_d)\n",
    "    for record_id, score in zip(id_set, scores) :\n",
    "        cluster_membership[record_id] = {\n",
    "            \"cluster id\" : cluster_id,\n",
    "            \"canonical representation\" : canonical_rep,\n",
    "            \"confidence\": score\n",
    "        }\n",
    "\n",
    "singleton_id = cluster_id + 1\n",
    "\n",
    "reader= df\n",
    "print(\"--------------------------------------------------------------------------outout\")\n",
    "ls=[]\n",
    "for row in reader.collect():\n",
    "    \n",
    "    row_id = (row[primart_key])\n",
    "    if row_id in cluster_membership :\n",
    "        cluster_id = cluster_membership[row_id][\"cluster id\"]\n",
    "        confidence_s=cluster_membership[row_id]['confidence']\n",
    "        g = str(float(\"{0:.2f}\".format(confidence_s)))\n",
    "        d_list=[row_id,cluster_id,g]\n",
    "\n",
    "        ls.append(d_list)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(row_id)\n",
    "        d_list=[row_id,singleton_id,'0']\n",
    "        singleton_id += 1\n",
    "        ls.append(d_list)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1498f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ls)\n",
    "columns = [\"row_id\", \"cluster_id\",\"confidence_score\"]    \n",
    "pr_str = \"reader.\" + primart_key\n",
    "dataframe = spark.createDataFrame(ls, columns,FloatType())\n",
    "\n",
    "\n",
    "\n",
    "new_df=reader.join(dataframe ,[reader.unique_id==  dataframe.row_id],\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bcca3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "adc197e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (94464538.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [81]\u001b[0;36m\u001b[0m\n\u001b[0;31m    new_df=reader.join(dataframe ,[reader{0}==  dataframe.row_id],\"inner\").format(\".\" + primart_key)\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "new_df=reader.join(dataframe ,[reader{}==  dataframe.row_id],\"inner\").format(\".\" + primart_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1d340d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a7204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91e5d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(\"/home/abhishekks446/data/big_basket_10k/output_9/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764e088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fffc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_str = \"reader.\" + primart_key + \" == dataframe.\" +\"row_id\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4ff73888",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=reader.join(dataframe ,eval(pr_str),\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4632e707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21637d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
