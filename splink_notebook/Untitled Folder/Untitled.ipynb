{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a483bcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/08/04 22:14:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/08/04 22:14:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from future.builtins import next\n",
    "import re\n",
    "from numpy import nan\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "import json\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType,FloatType\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dedupe\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51ed0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "InputCsvLocationFirst=\"/home/abhishekks446/data/output/10k_data.csv\"\n",
    "InputCsvLocationSecond=\"/home/abhishekks446/data/output/10k_data_2.csv\"\n",
    "PrimaryKey=\"unique_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b473b9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df1= spark.read.option(\"header\",\"true\").csv(InputCsvLocationFirst)\n",
    "df2= spark.read.option(\"header\",\"true\").csv(InputCsvLocationSecond)\n",
    "\n",
    "def preProcess(column):\n",
    "    \"\"\"\n",
    "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
    "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
    "    \"\"\"\n",
    "    import unidecode\n",
    "    # column = column.decode(\"utf8\")\n",
    "    column = unidecode.unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column :\n",
    "        column = None\n",
    "    return column\n",
    "def readData(df):\n",
    "    \"\"\"\n",
    "    Read in our data from a CSV file and create a dictionary of records,\n",
    "    where the key is a unique record ID and each value is dict\n",
    "    \"\"\"\n",
    "    \n",
    "    data_d = {}\n",
    "    dfn=df.na.fill(\"null\")\n",
    "    results = dfn.toJSON().map(lambda j: json.loads(j)).collect()\n",
    "    for row in results:\n",
    "        clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "        row_id = row[PrimaryKey]\n",
    "        data_d[row_id] = dict(clean_row)\n",
    "\n",
    "    return data_d\n",
    "\n",
    "DataFirst = readData(df1)\n",
    "DataSecond = readData(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a5f428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocked_ids = deduper.fingerprinter(DataFirst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc6ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "{'field' : 'store_name', 'type': 'String'},\n",
    "{'field' : 'zip_code', 'type': 'String'},\n",
    "{'field' : 'address', 'type': 'String'}\n",
    "]\n",
    "deduper = dedupe.Dedupe(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f8a1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "from boto3 import client\n",
    "\n",
    "\n",
    "BUCKET = 'steerwise-bucket'\n",
    "FILE_TO_READ = 'dedupe/input_data/labelled.json'\n",
    "client = client('s3',\n",
    "                 aws_access_key_id='AKIA5FYUQOE6KKEFWNYH',\n",
    "                 aws_secret_access_key='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'\n",
    "                )\n",
    "result = client.get_object(Bucket=BUCKET, Key=FILE_TO_READ) \n",
    "json_content = result[\"Body\"]\n",
    "\n",
    "# data = json.loads(json_content)\n",
    "deduper.prepare_training(DataFirst, json_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed02eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d038042",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = deduper.pairs(DataFirst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "868c0171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object DedupeMatching.pairs at 0x7f4f29e8b4a0>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ad8b5fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dedupe' object has no attribute 'scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mdeduper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscores\u001b[49m(pairs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dedupe' object has no attribute 'scores'"
     ]
    }
   ],
   "source": [
    "scores = deduper.scores(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17a5f597",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "score() missing 2 required positional arguments: 'self' and 'pairs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdedupe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDedupe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: score() missing 2 required positional arguments: 'self' and 'pairs'"
     ]
    }
   ],
   "source": [
    "dedupe.Dedupe.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d8685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02548e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152de807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e105d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper = dedupe.RecordLink(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dff327",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.prepare_training(DataFirst,DataSecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceff6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupe.console_label(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e4221",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/abhishekks446/data/output/dualdata_label.json\", 'w') as tf:\n",
    "    deduper.write_training(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b82b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper = dedupe.RecordLink(ListFields)\n",
    "deduper.prepare_training(DataFirst,DataSecond, labeled_examples)\n",
    "deduper.train()\n",
    "clustered_dupes=deduper.join(DataFirst,DataSecond, threshold=0.5,constraint=Constraint)\n",
    "\n",
    "lis=[list(elem) for elem in clustered_dupes]\n",
    "ls=[]\n",
    "for row in lis:\n",
    "    a=list(row[0])\n",
    "    c=[str(a[0]),str(a[1]),float(row[1])]\n",
    "    ls.append(c)\n",
    "columns = [\"df1\", \"df2\",\"confidence_score\"]    \n",
    "dataframe = spark.createDataFrame(ls,columns)\n",
    "dataframe.coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(Output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
