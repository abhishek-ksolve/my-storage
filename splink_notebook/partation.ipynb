{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd861151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f54e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/06/29 18:13:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb753fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import portable_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520441b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+\n",
      "|Amount|Country| ID|\n",
      "+------+-------+---+\n",
      "|    11|     AU|  1|\n",
      "|    12|     US|  2|\n",
      "|    13|     CN|  3|\n",
      "|    14|     AU|  4|\n",
      "|    15|     US|  5|\n",
      "|    16|     CN|  6|\n",
      "|    17|     AU|  7|\n",
      "|    18|     US|  8|\n",
      "|    19|     CN|  9|\n",
      "|    20|     AU| 10|\n",
      "|    21|     US| 11|\n",
      "|    22|     CN| 12|\n",
      "+------+-------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "countries = (\"CN\", \"AU\", \"US\")\n",
    "data = []\n",
    "for i in range(1, 13):\n",
    "    data.append({\"ID\": i, \"Country\": countries[i % 3],  \"Amount\": 10+i})\n",
    " \n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efefd65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_partitions(df):\n",
    "    numPartitions = df.rdd.getNumPartitions()\n",
    "    print(\"Total partitions: {}\\n\".format(numPartitions))\n",
    "    print(\"Partitioner: {}\\n\".format(df.rdd.partitioner))\n",
    "    df.explain()\n",
    "    print(\"\\n\")\n",
    "    parts = df.rdd.glom().collect()\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for p in parts:\n",
    "        print(\"\\nPartition {}:\".format(i))\n",
    "        for r in p:\n",
    "            print(\"Row {}:{}\".format(j, r))\n",
    "            j = j+1\n",
    "        i = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526b0d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 3\n",
      "\n",
      "Partitioner: None\n",
      "\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(Country#1, 3), REPARTITION_WITH_NUM, [id=#29]\n",
      "+- *(1) Scan ExistingRDD[Amount#0L,Country#1,ID#2L]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Partition 0:\n",
      "\n",
      "Partition 1:\n",
      "Row 0:Row(Amount=12, Country='US', ID=2)\n",
      "Row 1:Row(Amount=13, Country='CN', ID=3)\n",
      "Row 2:Row(Amount=15, Country='US', ID=5)\n",
      "Row 3:Row(Amount=16, Country='CN', ID=6)\n",
      "Row 4:Row(Amount=18, Country='US', ID=8)\n",
      "Row 5:Row(Amount=19, Country='CN', ID=9)\n",
      "Row 6:Row(Amount=21, Country='US', ID=11)\n",
      "Row 7:Row(Amount=22, Country='CN', ID=12)\n",
      "\n",
      "Partition 2:\n",
      "Row 8:Row(Amount=11, Country='AU', ID=1)\n",
      "Row 9:Row(Amount=14, Country='AU', ID=4)\n",
      "Row 10:Row(Amount=17, Country='AU', ID=7)\n",
      "Row 11:Row(Amount=20, Country='AU', ID=10)\n"
     ]
    }
   ],
   "source": [
    "df = df.repartition(3, \"Country\")\n",
    "print_partitions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7267ef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+--------------------+----------+\n",
      "|Amount|Country| ID|               Hash#|Partition#|\n",
      "+------+-------+---+--------------------+----------+\n",
      "|    12|     US|  2|-8328537658613580243|      -1.0|\n",
      "|    13|     CN|  3|-7458853143580063552|      -1.0|\n",
      "|    15|     US|  5|-8328537658613580243|      -1.0|\n",
      "|    16|     CN|  6|-7458853143580063552|      -1.0|\n",
      "|    18|     US|  8|-8328537658613580243|      -1.0|\n",
      "|    19|     CN|  9|-7458853143580063552|      -1.0|\n",
      "|    21|     US| 11|-8328537658613580243|      -1.0|\n",
      "|    22|     CN| 12|-7458853143580063552|      -1.0|\n",
      "|    11|     AU|  1| 6593628092971972691|       0.0|\n",
      "|    14|     AU|  4| 6593628092971972691|       0.0|\n",
      "|    17|     AU|  7| 6593628092971972691|       0.0|\n",
      "|    20|     AU| 10| 6593628092971972691|       0.0|\n",
      "+------+-------+---+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F  \n",
    "from pyspark.sql import udf  \n",
    "udf_portable_hash = F.udf(lambda str: portable_hash(str))\n",
    "df = df.withColumn(\"Hash#\", udf_portable_hash(df.Country))\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % 3)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd0de5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+-----+----------+\n",
      "|Amount|Country| ID|Hash#|Partition#|\n",
      "+------+-------+---+-----+----------+\n",
      "|    20|     AU| 10|    1|       1.0|\n",
      "|    11|     AU|  1|    1|       1.0|\n",
      "|    14|     AU|  4|    1|       1.0|\n",
      "|    17|     AU|  7|    1|       1.0|\n",
      "|    16|     CN|  6|    0|       0.0|\n",
      "|    13|     CN|  3|    0|       0.0|\n",
      "|    22|     CN| 12|    0|       0.0|\n",
      "|    19|     CN|  9|    0|       0.0|\n",
      "|    12|     US|  2|    2|       2.0|\n",
      "|    18|     US|  8|    2|       2.0|\n",
      "|    21|     US| 11|    2|       2.0|\n",
      "|    15|     US|  5|    2|       2.0|\n",
      "+------+-------+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = (\"CN\", \"AU\", \"US\")\n",
    "def country_partitioning(k):\n",
    "    return countries.index(k)\n",
    "    \n",
    "udf_country_hash = F.udf(lambda str: country_partitioning(str))\n",
    "numPartitions = 3\n",
    "# df = df.partitionBy(numPartitions, country_partitioning)\n",
    "df = df.withColumn(\"Hash#\", udf_country_hash(df['Country']))\n",
    "df = df.withColumn(\"Partition#\", df[\"Hash#\"] % numPartitions)\n",
    "df.orderBy('Country').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0be3f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total partitions: 3\n",
      "\n",
      "Partitioner: None\n",
      "\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(Partition##60, 3), REPARTITION_WITH_NUM, [id=#105]\n",
      "+- *(2) Project [Amount#0L, Country#1, ID#2L, pythonUDF0#88 AS Hash##54, (cast(pythonUDF0#88 as double) % 3.0) AS Partition##60]\n",
      "   +- BatchEvalPython [<lambda>(Country#1)], [pythonUDF0#88]\n",
      "      +- Exchange hashpartitioning(Country#1, 3), REPARTITION_WITH_NUM, [id=#100]\n",
      "         +- *(1) Scan ExistingRDD[Amount#0L,Country#1,ID#2L]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Partition 0:\n",
      "Row 0:Row(Amount=12, Country='US', ID=2, Hash#='2', Partition#=2.0)\n",
      "Row 1:Row(Amount=15, Country='US', ID=5, Hash#='2', Partition#=2.0)\n",
      "Row 2:Row(Amount=18, Country='US', ID=8, Hash#='2', Partition#=2.0)\n",
      "Row 3:Row(Amount=21, Country='US', ID=11, Hash#='2', Partition#=2.0)\n",
      "\n",
      "Partition 1:\n",
      "Row 4:Row(Amount=13, Country='CN', ID=3, Hash#='0', Partition#=0.0)\n",
      "Row 5:Row(Amount=16, Country='CN', ID=6, Hash#='0', Partition#=0.0)\n",
      "Row 6:Row(Amount=19, Country='CN', ID=9, Hash#='0', Partition#=0.0)\n",
      "Row 7:Row(Amount=22, Country='CN', ID=12, Hash#='0', Partition#=0.0)\n",
      "\n",
      "Partition 2:\n",
      "Row 8:Row(Amount=11, Country='AU', ID=1, Hash#='1', Partition#=1.0)\n",
      "Row 9:Row(Amount=14, Country='AU', ID=4, Hash#='1', Partition#=1.0)\n",
      "Row 10:Row(Amount=17, Country='AU', ID=7, Hash#='1', Partition#=1.0)\n",
      "Row 11:Row(Amount=20, Country='AU', ID=10, Hash#='1', Partition#=1.0)\n"
     ]
    }
   ],
   "source": [
    "print_partitions(df.repartition(3, \"Partition#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e0c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
